{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import logging\n",
    "import logging.config\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "from tral import configuration\n",
    "from tral.paths import config_file\n",
    "from tral.repeat_list import repeat_list\n",
    "from tral.sequence import sequence\n",
    "from tral.hmm import hmm\n",
    "from tral.hmm import hmm_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfaidx\n",
      "  Downloading https://files.pythonhosted.org/packages/75/a5/7e2569527b3849ea28d79b4f70d7cf46a47d36459bc59e0efa4e10e8c8b2/pyfaidx-0.5.5.2.tar.gz\n",
      "Requirement already satisfied: six in /home/lina/anaconda3/envs/test_tral/lib/python3.7/site-packages (from pyfaidx) (1.11.0)\n",
      "Requirement already satisfied: setuptools>=0.7 in /home/lina/anaconda3/envs/test_tral/lib/python3.7/site-packages (from pyfaidx) (40.5.0)\n",
      "Building wheels for collected packages: pyfaidx\n",
      "  Running setup.py bdist_wheel for pyfaidx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/lina/.cache/pip/wheels/54/a2/b4/e242e58d23b2808e191b214067880faa46cd2341f363886e0b\n",
      "Successfully built pyfaidx\n",
      "Installing collected packages: pyfaidx\n",
      "Successfully installed pyfaidx-0.5.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyfaidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.config.fileConfig(config_file(\"logging.ini\"))\n",
    "LOG = logging.getLogger('root')\n",
    "\n",
    "CONFIG = configuration.Configuration.instance().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift REPEAT_LIST_TAG to configuration file and rename?\n",
    "REPEAT_LIST_TAG = \"all\"\n",
    "DE_NOVO_ALL_TAG = \"denovo_all\"\n",
    "PFAM_ALL_TAG = \"pfam_all\"\n",
    "DE_NOVO_TAG = \"denovo\"\n",
    "PFAM_TAG = \"pfam\"\n",
    "DE_NOVO_REFINED_TAG = \"denovo_refined\"\n",
    "DE_NOVO_FINAL_TAG = \"denovo_final\"\n",
    "FINAL_TAG = \"final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow(\n",
    "        sequences_file,\n",
    "        hmm_annotation_file,\n",
    "        hmm_dir,\n",
    "        result_file,\n",
    "        result_file_serialized,\n",
    "        format,\n",
    "        max_time,\n",
    "        time_interval=3600,\n",
    "        next_time=3600,\n",
    "        **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workflow(\n",
    "        sequences_file,\n",
    "        hmm_annotation_file,\n",
    "        hmm_dir,\n",
    "        result_file,\n",
    "        result_file_serialized,\n",
    "        format,\n",
    "        max_time,\n",
    "        time_interval=3600,\n",
    "        next_time=3600,\n",
    "        **kwargs):\n",
    "    ''' Annotate sequences with TRs from multiple sources, test and refine annotations.\n",
    "\n",
    "     Save the annotations in a pickle.\n",
    "\n",
    "     Args:\n",
    "         sequences_file (str): Path to the pickle file containing a list of ``Sequence``\n",
    "            instances.\n",
    "         hmm_dir (str): Path to directory where all HMMs are stored as .pickles\n",
    "         result_file (str): Path to the result file.\n",
    "         max_time (str): Max run time in seconds\n",
    "\n",
    "     Raises:\n",
    "        Exception: If the pickle ``sequences_file`` cannot be loaded\n",
    "        Exception: if the hmm_dir does not exist\n",
    "\n",
    "    '''\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    max_time, time_interval, next_time = int(\n",
    "        max_time), int(time_interval), int(next_time)\n",
    "\n",
    "    try:\n",
    "        l_sequence = Fasta(sequences_file)\n",
    "    except:\n",
    "        raise Exception(\n",
    "            \"Cannot load putative pickle file sequences_file: {}\".format(sequences_file))\n",
    "\n",
    "    if not os.path.isdir(hmm_dir):\n",
    "        try:\n",
    "            os.makedirs(hmm_dir)\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"hmm_dir does not exists and could not be created: {}\".format(hmm_dir))\n",
    "\n",
    "    try:\n",
    "        with open(hmm_annotation_file, 'rb') as fh:\n",
    "            dHMM_annotation = pickle.load(fh)\n",
    "    except:\n",
    "        raise Exception(\n",
    "            \"Cannot load hmm_annotation_file: {}\".format(hmm_annotation_file))\n",
    "\n",
    "    basic_filter = CONFIG['filter']['basic']['dict']\n",
    "    basic_filter_tag = CONFIG['filter']['basic']['tag']\n",
    "\n",
    "    # Load previous results\n",
    "    try:\n",
    "        if not os.path.isdir(os.path.dirname(result_file)):\n",
    "            os.makedirs(os.path.dirname(result_file))\n",
    "    except:\n",
    "        raise Exception(\n",
    "            \"Could not create path to result_file directory: {}\".format(\n",
    "                os.path.dirname(result_file)))\n",
    "\n",
    "    try:\n",
    "        with open(result_file, 'rb') as fh:\n",
    "            dResults = pickle.load(fh)\n",
    "    except:\n",
    "        LOG.debug(\n",
    "            \"Could not load previous results file - perhaps non existant: {}\".format(result_file))\n",
    "        dResults = {}\n",
    "\n",
    "    dHMM = {}\n",
    "    for iS_pyfaidx in l_sequence:\n",
    "\n",
    "        # If sequence is already included in results: continue.\n",
    "        if iS_pyfaidx.name in dResults:\n",
    "            continue\n",
    "\n",
    "        elapsed_time = (datetime.datetime.now() - start).seconds\n",
    "        if elapsed_time > max_time or elapsed_time > next_time:\n",
    "            with open(result_file, 'wb') as fh:\n",
    "                pickle.dump(dResults, fh)\n",
    "            next_time = next_time + time_interval\n",
    "\n",
    "        iS = sequence.Sequence(seq=str(iS_pyfaidx), name=iS_pyfaidx.name.split(\"|\")[1])\n",
    "\n",
    "        LOG.debug(\"Work on sequence {}\".format(iS))\n",
    "        # 1. annotate_de_novo()\n",
    "        denovo_repeat_list = iS.detect(\n",
    "            denovo=True,\n",
    "            repeat={\n",
    "                \"calc_pvalue\": True})\n",
    "        LOG.debug(denovo_repeat_list.repeats)\n",
    "        for iTR in denovo_repeat_list.repeats:\n",
    "            iTR.model = None\n",
    "\n",
    "        # 2. annotate_TRs_from_hmmer()\n",
    "        if iS.name in dHMM_annotation and len(dHMM_annotation[iS.name]) != 0:\n",
    "            lHMM = dHMM_annotation[iS.name]\n",
    "            infoNRuns = len(lHMM)\n",
    "            LOG.debug(\n",
    "                \"{} Viterbi runs need to be performed.\".format(infoNRuns))\n",
    "            lHMM = set(lHMM)\n",
    "            infoNHMM = len(lHMM)\n",
    "            LOG.debug(\n",
    "                \"These derive from {} independent HMMs.\".format(infoNHMM))\n",
    "            # Load all HMM pickles needed for the particular sequence.\n",
    "            for hmm_ID in lHMM:\n",
    "                if hmm_ID not in dHMM:\n",
    "                    dHMM[hmm_ID] = hmm.HMM.create(\n",
    "                        input_format=\"pickle\",\n",
    "                        file=os.path.join(\n",
    "                            hmm_dir,\n",
    "                            hmm_ID +\n",
    "                            \".pickle\"))\n",
    "\n",
    "            pfam_repeat_list = iS.detect(\n",
    "                [dHMM[hmm_ID] for hmm_ID in lHMM], repeat={\"calc_pvalue\": True})\n",
    "            for iTR, hmm_ID in zip(pfam_repeat_list.repeats, lHMM):\n",
    "                iTR.model = hmm_ID\n",
    "                iTR.TRD = \"PFAM\"\n",
    "        else:\n",
    "            pfam_repeat_list = None\n",
    "\n",
    "        # 3. merge_and_basic_filter()\n",
    "        all_repeat_list = denovo_repeat_list + pfam_repeat_list\n",
    "        iS.set_repeatlist(all_repeat_list, REPEAT_LIST_TAG)\n",
    "        iS.set_repeatlist(denovo_repeat_list, DE_NOVO_ALL_TAG)\n",
    "        iS.set_repeatlist(pfam_repeat_list, PFAM_ALL_TAG)\n",
    "\n",
    "        rl_tmp = iS.get_repeatlist(REPEAT_LIST_TAG)\n",
    "        if iS.get_repeatlist(REPEAT_LIST_TAG):\n",
    "            for iB in basic_filter.values():\n",
    "                rl_tmp = rl_tmp.filter(**iB)\n",
    "        else:\n",
    "            rl_tmp = iS.get_repeatlist(REPEAT_LIST_TAG)\n",
    "        iS.set_repeatlist(rl_tmp, basic_filter_tag)\n",
    "        iS.set_repeatlist(\n",
    "            rl_tmp.intersection(\n",
    "                iS.get_repeatlist(PFAM_ALL_TAG)), PFAM_TAG)\n",
    "        iS.set_repeatlist(\n",
    "            rl_tmp.intersection(iS.get_repeatlist(DE_NOVO_ALL_TAG)),DE_NOVO_TAG)\n",
    "\n",
    "        # 4. calculate_overlap()\n",
    "\n",
    "        # Perform common ancestry overlap filter and keep PFAMs\n",
    "        criterion_pfam_fixed = {\n",
    "            \"func_name\": \"none_overlapping_fixed_repeats\",\n",
    "            \"rl_fixed\": iS.get_repeatlist(PFAM_TAG),\n",
    "            \"overlap_type\": \"common_ancestry\"}\n",
    "\n",
    "        iS.d_repeatlist[DE_NOVO_TAG] = iS.get_repeatlist(DE_NOVO_TAG).filter(**criterion_pfam_fixed)\n",
    "\n",
    "        # Choose only the most convincing de novo TRs\n",
    "        criterion_filter_order = {\n",
    "            \"func_name\": \"none_overlapping\", \"overlap\": (\n",
    "                \"common_ancestry\", None), \"l_criterion\": [\n",
    "                (\"pvalue\", \"phylo_gap01\"), (\"divergence\", \"phylo_gap01\")]}\n",
    "        iS.d_repeatlist[DE_NOVO_TAG] = iS.get_repeatlist(DE_NOVO_TAG).filter(**criterion_filter_order)\n",
    "\n",
    "        # 5. refine_denovo()\n",
    "        denovo_final = []\n",
    "        denovo_refined = [None] * len(iS.get_repeatlist(DE_NOVO_ALL_TAG).repeats)\n",
    "        for i, iTR in enumerate(iS.get_repeatlist(DE_NOVO_ALL_TAG).repeats):\n",
    "            if not iTR in iS.get_repeatlist(DE_NOVO_TAG).repeats:\n",
    "                continue\n",
    "            # Create HMM from TR\n",
    "            denovo_hmm = hmm.HMM.create(input_format='repeat', repeat=iTR)\n",
    "            # Run HMM on sequence\n",
    "            denovo_refined_rl = iS.detect(lHMM=[denovo_hmm])\n",
    "            append_refined = False\n",
    "            if denovo_refined_rl and denovo_refined_rl.repeats:\n",
    "                iTR_refined = denovo_refined_rl.repeats[0]\n",
    "                iTR_refined.TRD = iTR.TRD\n",
    "                iTR_refined.model = \"cpHMM\"\n",
    "                denovo_refined[i] = iTR_refined\n",
    "                # Check whether new and old TR overlap. Check whether new TR is\n",
    "                # significant. If not both, put unrefined TR into final.\n",
    "                if repeat_list.two_repeats_overlap(\n",
    "                        \"shared_char\",\n",
    "                        iTR,\n",
    "                        iTR_refined):\n",
    "                    rl_tmp = repeat_list.RepeatList([iTR_refined])\n",
    "                    LOG.debug(iTR_refined.msa)\n",
    "                    for iB in basic_filter.values():\n",
    "                        rl_tmp = rl_tmp.filter(**iB)\n",
    "                    if rl_tmp.repeats:\n",
    "                        append_refined = True\n",
    "            else:\n",
    "                denovo_refined[i] = False\n",
    "            if append_refined:\n",
    "                denovo_final.append(iTR_refined)\n",
    "            else:\n",
    "                denovo_final.append(iTR)\n",
    "\n",
    "        iS.set_repeatlist(\n",
    "            repeat_list.RepeatList(denovo_refined),\n",
    "            DE_NOVO_REFINED_TAG)\n",
    "        iS.set_repeatlist(\n",
    "            repeat_list.RepeatList(denovo_final),\n",
    "            DE_NOVO_FINAL_TAG)\n",
    "        iS.set_repeatlist(\n",
    "            iS.get_repeatlist(DE_NOVO_FINAL_TAG) +\n",
    "            iS.get_repeatlist(PFAM_TAG),\n",
    "            FINAL_TAG)\n",
    "\n",
    "        dResults[iS.name] = iS\n",
    "\n",
    "    # 6.a Save results as pickle\n",
    "    with open(result_file, 'wb') as fh:\n",
    "        pickle.dump(dResults, fh)\n",
    "\n",
    "    # 6.b Save serialized results\n",
    "    with open(result_file_serialized, 'w') as fh_o:\n",
    "\n",
    "        if format == 'tsv':\n",
    "            header = [\n",
    "                \"ID\",\n",
    "                \"MSA\",\n",
    "                \"begin\",\n",
    "                \"pvalue\",\n",
    "                \"l_effective\",\n",
    "                \"n\",\n",
    "                \"n_effective\",\n",
    "                \"TRD\",\n",
    "                \"model\"]\n",
    "        fh_o.write(\"\\t\".join(header))\n",
    "\n",
    "        for iS in dResults.values():\n",
    "            for iTR in iS.get_repeatlist(FINAL_TAG).repeats:\n",
    "                if format == 'tsv':\n",
    "                    try:\n",
    "                        data = [\n",
    "                            str(i) for i in [\n",
    "                                iS.name,\n",
    "                                \" \".join(\n",
    "                                    iTR.msa),\n",
    "                                iTR.begin,\n",
    "                                iTR.pvalue(\"phylo_gap01\"),\n",
    "                                iTR.l_effective,\n",
    "                                iTR.n,\n",
    "                                iTR.n_effective,\n",
    "                                iTR.TRD,\n",
    "                                iTR.model]]\n",
    "                    except:\n",
    "                        print(iTR)\n",
    "                        raise Exception(\n",
    "                            \"(Could not save data for the above TR.)\")\n",
    "\n",
    "                fh_o.write(\"\\n\" + \"\\t\".join(data))\n",
    "\n",
    "    print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_preparation(\n",
    "        annotation_data_file,\n",
    "        annotation_data_output,\n",
    "        hmm_raw_file,\n",
    "        hmm_dir):\n",
    "\n",
    "    create_hmm_files(hmm_raw_file, hmm_dir)\n",
    "\n",
    "    read_pfam_uniprot(annotation_data_file, annotation_data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hmm_files(hmm_file, output_file):\n",
    "\n",
    "    for hmmer_probabilities in hmm_io.read(hmm_file):\n",
    "        iID = hmmer_probabilities['id'].split(\".\")[0]\n",
    "        iHMM = hmm.HMM(hmmer_probabilities)\n",
    "        file = os.path.join(output_file, \"{}.pickle\".format(iID))\n",
    "        iHMM.write(file, \"pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pfam_uniprot(annotation_data_file, output_file, csv_delimiter=\"\\t\", annotation_delimiter=\";\"):\n",
    "    ''' annotation_file from:\n",
    "        http://www.uniprot.org/uniprot/?query=database:(type:pfam%20AND%20*)&fil=&sort=score '''\n",
    "\n",
    "    p = {}\n",
    "    if annotation_data_file:\n",
    "        try:\n",
    "            with open(annotation_data_file) as f:\n",
    "                reader = csv.reader(f, delimiter=csv_delimiter)\n",
    "                for row in reader:\n",
    "                    p[row[0]] = [i for i in row[1].split(annotation_delimiter) if i != \"\"]\n",
    "        except:\n",
    "            raise Exception(\"Cannot load sequence annotation file annotation_data_file: {}\".format(\n",
    "                annotation_data_file))\n",
    "\n",
    "    with open(output_file, 'wb') as fh:\n",
    "        pickle.dump(p, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_csv_files(directory, result_file, file_extension=\".csv\"):\n",
    "    files = [file for file in os.listdir(directory) if file.endswith(file_extension)]\n",
    "    shutil.copyfile(os.path.join(directory, files.pop()), result_file)\n",
    "    with open(result_file, \"a\") as fh:\n",
    "        for file in files:\n",
    "            fh.write(\"\\n\")\n",
    "            with open(os.path.join(directory, file), \"r\") as fh2:\n",
    "                fh2.readline()\n",
    "                for line in fh2.readlines():\n",
    "                    fh.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    pars = read_commandline_arguments()\n",
    "\n",
    "    # Update configuration\n",
    "    if \"sequence_type\" in pars:\n",
    "        CONFIG[\"sequence_type\"] = pars[\"sequence_type\"]\n",
    "    if \"detectors\" in pars:\n",
    "        CONFIG[\"sequence\"][\"repeat_detection\"][\n",
    "            CONFIG[\"sequence_type\"]] = pars[\"detectors\"]\n",
    "    if \"significance_test\" in pars:\n",
    "        CONFIG[\"repeat_score\"][\"score_calibration\"] = pars[\"significance_test\"]\n",
    "\n",
    "    if pars[\"method\"] == \"file_preparation\":\n",
    "        file_preparation(\n",
    "            pars[\"hmm_annotation_raw\"],\n",
    "            pars[\"hmm_annotation\"],\n",
    "            pars[\"hmm_raw\"],\n",
    "            pars[\"hmm\"])\n",
    "    elif pars[\"method\"] == \"workflow\":\n",
    "        workflow(\n",
    "            pars[\"input\"],\n",
    "            pars[\"hmm_annotation\"],\n",
    "            pars[\"hmm\"],\n",
    "            pars[\"output\"],\n",
    "            pars[\"output_serialized\"],\n",
    "            pars[\"format\"],\n",
    "            pars[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_commandline_arguments():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Process tandem repeat detection options')\n",
    "    parser.add_argument('method', metavar='method_name', type=str,\n",
    "                        help='The name of the method to be executed.')\n",
    "    parser.add_argument('-i', '--input', type=str,\n",
    "                        help='The path to the input file.')\n",
    "    parser.add_argument('-o', '--output', type=str,\n",
    "                        help='The path to the output file.')\n",
    "    parser.add_argument('-os', '--output_serialized',\n",
    "                        help='The path to the serialized output file.')\n",
    "    parser.add_argument('-f', '--format', type=str,\n",
    "                        help='The serialization format, e.g. tsv')\n",
    "    parser.add_argument('-t', '--time', type=str,\n",
    "                        help='The maximum runtime')\n",
    "    parser.add_argument('-hmm', '--hmm', type=str,\n",
    "                        help='The path to the dir with HMM pickles')\n",
    "    parser.add_argument('-hmm_raw', '--hmm_raw', type=str,\n",
    "                        help='The path to the raw file with HMM models')\n",
    "    parser.add_argument(\n",
    "        '-hmm_annotation',\n",
    "        '--hmm_annotation',\n",
    "        type=str,\n",
    "        help='The path the pickle with sequence to HMM mappings')\n",
    "    parser.add_argument(\n",
    "        '-hmm_annotation_raw',\n",
    "        '--hmm_annotation_raw',\n",
    "        type=str,\n",
    "        help='The path the raw file with sequence to HMM mappings')\n",
    "\n",
    "    pars = vars(parser.parse_args())\n",
    "    pars = {key: value for key, value in pars.items() if value is not None}\n",
    "    return pars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
